{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e722c-7ffb-47a2-9cba-494eda527e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "from mastodon import Mastodon\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42372f-e1cd-4193-bc07-33f9514f71ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Crawling and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071aa6c-1a97-42e4-94cb-f0802988b7ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Login and Test Mastodon API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f8084-5841-4092-b973-31a8a6d47319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mastodon.create_app(\n",
    "#     'crawler',\n",
    "#     api_base_url = 'https://mastodon.social',\n",
    "#     to_file = 'clientcred.secret'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6df6a1-7e26-4f6d-bf60-c340d7adc5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mastodon = Mastodon(client_id = 'clientcred.secret',)\n",
    "mastodon.log_in(\n",
    "    'e0925463@u.nus.edu',\n",
    "    'social_scope',\n",
    "    to_file = 'usercred.secret'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c925c8c-2927-4dd9-b2f5-f2720a6d29f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mastodon.status(\"109355825433550347\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878408f-4b67-43ee-90ff-4f205637c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: maximum limit of status is only 40\n",
    "statuses = mastodon.account_statuses(\"108199052370473764\", limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ef082-6a98-4a6e-bc7d-4d757e4cc37e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Crawl User Statuses/Toots Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db20a29-b2a6-4d86-9f0b-119456afe99a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_id = \"108199052370473764\"\n",
    "\n",
    "min_date = datetime(2022,5,1,0,0,0, tzinfo=tzlocal())\n",
    "max_date = datetime(2023,2,1,0,0,0, tzinfo=tzlocal())\n",
    "curr_date = min_date\n",
    "done_ind = False\n",
    "\n",
    "df = pd.DataFrame(columns=['user_id','status_created_datetime','status_id', 'status_uri','status_url','status_content_raw',\n",
    "                           'reblog',\n",
    "                           'reblog_user_id','reblog_created_datetime','reblog_id','reblog_uri','reblog_url','reblog_content_raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf162190-41e5-4dac-b7f0-1759f23aa9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_list = []\n",
    "\n",
    "while not(done_ind):\n",
    "    statuses = mastodon.account_statuses(user_id, min_id=curr_date, max_id=max_date, limit=40)\n",
    "    \n",
    "    if statuses == []:\n",
    "        curr_date = curr_date + timedelta(days=40)\n",
    "    else:\n",
    "        for status in reversed(statuses):\n",
    "            row_dict = {}\n",
    "            if status['created_at'] < max_date:\n",
    "                row_dict = {\n",
    "                                'user_id': user_id,\n",
    "                                'status_created_datetime': status['created_at'],\n",
    "                                'status_id': status['id'], \n",
    "                                'status_uri': status['uri'],\n",
    "                                'status_url': status['url'],\n",
    "                                'status_content_raw': status['content'],\n",
    "                                'reblog': False\n",
    "                }\n",
    "\n",
    "                if not(status['reblog'] is None):\n",
    "                    row_dict['reblog'] = True\n",
    "                    row_dict['reblog_user_id'] = status['reblog']['account']['id']\n",
    "                    row_dict['reblog_created_datetime'] = status['reblog']['created_at']\n",
    "                    row_dict['reblog_id'] = status['reblog']['id']\n",
    "                    row_dict['reblog_uri'] = status['reblog']['uri']\n",
    "                    row_dict['reblog_url'] = status['reblog']['url']\n",
    "                    row_dict['reblog_content_raw'] = status['reblog']['content']\n",
    "\n",
    "                curr_date = status['created_at']\n",
    "\n",
    "                df_dict_list.append(row_dict)\n",
    "\n",
    "            else:\n",
    "                done_ind = True\n",
    "                break\n",
    "    \n",
    "    if len(statuses) < 40:\n",
    "        done_ind = True\n",
    "    \n",
    "    if curr_date >= max_date:\n",
    "        done_ind = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309b9ad-c3b6-4d48-8609-72cea522107a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(df_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc141cb-ea2b-4217-9df3-815af5014ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208aacfb-833b-4d8c-aa7e-71ab7e452dfb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tokenizing (a single) Status/Toot - and Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121b44e-b046-4a6b-9a26-d87bcda56b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_html_tokenizer import create_html_tokenizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf604f06-4e0c-4daf-8fc4-f9c8ba35df83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = create_html_tokenizer()(nlp)\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "sample_html = 'The wild backstory of how Trump pitted his own social media company against another company run by a Trump adviser to get a more lucrative deal for himself: '\n",
    "doc = nlp(sample_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172dab43-9d22-4edf-83d8-0f86a54dda93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(re.sub(r'http\\S+', '', sent.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1451ea70-2645-444c-85c6-e8335bed7551",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenizing Whole Raw Content Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc8ba6-2b86-41b7-ab70-97263e6d92d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = create_html_tokenizer()(nlp)\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "def html_tokenize(html_text, nlp=nlp):\n",
    "    res = \"\"\n",
    "    if html_text == \"\" or html_text is None or not(type(html_text)==str):\n",
    "        return None\n",
    "    else:\n",
    "        doc = nlp(html_text)\n",
    "        for sent in doc.sents:\n",
    "            res += \" \" + re.sub(r'http\\S+', '', sent.text)\n",
    "        return res[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cc2ef-e480-4302-943e-26e4b62b8450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status_content = [html_tokenize(text) for text in df['status_content_raw']]\n",
    "reblog_content = [html_tokenize(text) for text in df['reblog_content_raw']]\n",
    "\n",
    "df['status_content'] = status_content\n",
    "df['reblog_content'] = reblog_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f07acd-b2d9-4c1c-a125-b4ea4537d693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bae969-4f7a-4cce-92de-489e36d652d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine Tuning GPT3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9797c29-7262-4854-84f7-26b79339bafb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e98ae-7474-44a9-b9c7-f3e297a33fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c2fc5-a6d0-4d0e-af88-d4c5801c3144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: Who won the 2020 Summer Olympics men's high jump?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a714b-1ae8-4f28-a13b-13e80034610a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188af94-9ab2-4f63-9115-37696a23f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL) -> list[float]:\n",
    "    if text is not None:\n",
    "        try:\n",
    "            result = openai.Embedding.create(\n",
    "              model=model,\n",
    "              input=text\n",
    "            )\n",
    "            return result[\"data\"][0][\"embedding\"]\n",
    "        except:\n",
    "            return \"Rate Limit Reached\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame, label:str) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    for idx, r in tqdm(df.iterrows()):\n",
    "        embedding = get_embedding(r[label])\n",
    "        if type(embedding) == str:\n",
    "            time.sleep(60)\n",
    "            embedding = get_embedding(r[label])\n",
    "        \n",
    "        embeddings_dict[idx] = embedding\n",
    "    \n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864c0c1-6d69-4c06-854e-ccdc26f901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = compute_doc_embeddings(df, label=\"status_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30003ff-112a-4661-8c45-ef35d32dee6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save embeddings as pickle\n",
    "with open('doc_embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump(document_embeddings, f, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b6ef0-f5f6-4c06-b64e-c5368a4bae12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finding Similar Documents Using Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99814d87-39d9-47cf-8e7a-99403e58e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: list[float], y: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.\n",
    "    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    if x is not None and y is not None:\n",
    "        return np.dot(np.array(x), np.array(y))\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -> list[(float, (str, str))]:\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0328c2c-03cd-47b4-babd-2f524f7700a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_document_sections_by_query_similarity(\"What was Trump's wild story?\", document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282102a-3a43-4026-955f-12776588702a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Construct Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f3acc-68c9-470c-8136-048361cb827e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n* \"\n",
    "ENCODING = \"gpt2\"  # encoding for text-davinci-003\n",
    "\n",
    "encoding = tiktoken.get_encoding(ENCODING)\n",
    "separator_len = len(encoding.encode(SEPARATOR))\n",
    "\n",
    "f\"Context separator contains {separator_len} tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321193c5-fa2e-40fa-85db-ed56f2cc53ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame, label:str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant docs\n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = df.loc[section_index][label]\n",
    "        \n",
    "        if document_section is None:\n",
    "            continue\n",
    "        \n",
    "        chosen_sections_len += num_tokens_from_string(document_section, ENCODING) + separator_len\n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section.replace(\"\\n\", \" \"))\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefc42c-5b23-437a-90bf-9996162dd207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = construct_prompt(\n",
    "    \"What was Trump's wild story?\",\n",
    "    document_embeddings,\n",
    "    df,\n",
    "    label=\"status_content\"\n",
    ")\n",
    "\n",
    "print(\"===\\n\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc166c6-3b8f-4c38-b71d-c019e0f43645",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Answer Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0f83f-25d3-47de-852f-db15d630e969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 300,\n",
    "    \"model\": COMPLETIONS_MODEL,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820f6ca-a910-4827-9b2b-e2a4b3298f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_query_with_context(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    document_embeddings: dict[(str, str), np.array],\n",
    "    label:str,\n",
    "    show_prompt: bool = False,\n",
    ") -> str:\n",
    "    prompt = construct_prompt(\n",
    "        query,\n",
    "        document_embeddings,\n",
    "        df,\n",
    "        label=label\n",
    "    )\n",
    "    \n",
    "    if show_prompt:\n",
    "        print(prompt)\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "                prompt=prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2356103-a818-4845-815a-e40cc0fe5b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('doc_embeddings.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "    \n",
    "answer_query_with_context(\"What happned to facebook?\", df, document_embeddings, label=\"status_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4a157-5c84-4129-b2d2-2376803045b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[119]['status_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7a808-42bd-427f-8ca0-406f8cab4242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[3]['status_content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
